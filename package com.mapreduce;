package com.mapreduce;

import java.io.IOException;
import java.util.Date;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;

import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Mapper.Context;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.joda.time.DateTime;

public class DataPreprocessing {

	public static class TokenizerMapper extends Mapper<Object, Text, Text, IntWritable> {

		private DateTime dt;
		private String intermediate;
		private int weekOfYear, monthOfYear;
		private Text word = new Text();

		public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
			String tempString = value.toString();
			String[] columnData = tempString.split(",");
			// jdkDate = new Date(columnData[0]);
			context.write(new Text(columnData[0]), new IntWritable(1));

		}
	}

	public static class MyIntSumReducer extends Reducer<Text, Iterable<Text>, Text, IntWritable> {
		private DateTime dt;
		private String intermediate;
		private int weekOfYear, monthOfYear;
		private float openValue,closeValue,highValue,lowValue;
		private float sum;
		private String divider[];
		private Text result;
		private IntWritable result1 = new IntWritable();

		public void reduce(Text key,Iterable<Text> values, Context context) throws IOException, InterruptedException {
			int total=0;
			

				// jdkDate = new Date(columnData[0]);
//				dt = new DateTime(key.toString());
//				weekOfYear = dt.getWeekOfWeekyear();
//				monthOfYear = dt.getMonthOfYear();
//				intermediate = String.valueOf(weekOfYear) + " " + String.valueOf(monthOfYear);

			//intermediate = values.toString();
		//	divider = intermediate.split(",");
			//sum = Float.parseFloat(divider[0])+Float.parseFloat(divider[1]);
			//result = new Text(String.valueOf(sum));
			for(Text val:values){
				total++;
			}
			result1.set(total);
			context.write(key,result1);
			
		}
	}

	public static void main(String[] args) throws Exception {
		Configuration conf = new Configuration();
		Job job = Job.getInstance(conf, "word count");
		job.setJarByClass(DataPreprocessing.class);
		job.setMapperClass(TokenizerMapper.class);
		job.setCombinerClass(MyIntSumReducer.class);
		job.setReducerClass(MyIntSumReducer.class);
		job.setMapOutputKeyClass(Text.class);
		job.setMapOutputValueClass(IntWritable.class);
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(IntWritable.class);
		FileInputFormat.addInputPath(job, new Path(args[0]));
		FileOutputFormat.setOutputPath(job, new Path(args[1]));
		System.exit(job.waitForCompletion(true) ? 0 : 1);


		//		 Job mapAConf = job.getInstance(conf);
//	     ChainMapper.addMapper(mapAConf, TokenizerMapper.class, Text.class, Text.class, Text.class, Text.class, conf);      
//	        
//	            //addMapper will take global conf object and mapper class ,input and output type for this mapper and output key/value have to be sent by value or by reference and localJObconf specific to this call
//	       
//	     Job mapBConf = job.getInstance(conf);
//	     ChainMapper.addMapper(mapBConf, DateGeneratorMapper.class, Text.class, Text.class, Text.class, Text.class, conf);
//
//	    Job reduceConf = job.getInstance(conf);
//	     ChainReducer.setReducer(reduceConf, IntSumReducer.class, Text.class, Text.class, Text.class, Text.class, conf);

//	       JobConf mapCConf = new JobConf(false);
//	       ChainReducer.addMapper(conf, LastMapper.class, Text.class, IntWritable.class, Text.class, IntWritable.class, true, mapCConf);
////		Configuration conf1 = new Configuration();
		//Configuration conf1 = new Configuration();
	}
}
